import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense, Dropout
import pandas as pd
from sklearn.feature_selection import mutual_info_classif
import boto3
import botocore

s3_client=boto3.client('s3')

def data_process(train_X,dependant,train_Y):
    data=train_X
    train_Y=train_Y

    dependant=dependant
    data[dependant]=train_Y

    train,test=train_test_split(data,test_size=0.01,shuffle=True)

    train_Y=train[dependant]
    del train[dependant]

    train_X= train

    test_Y=test[dependant]
    del test[dependant]

    test_X=test

    # Data needs to be scaled to a small range like 0 to 1 for the neural
    # network to work well.
    scalerX = MinMaxScaler(feature_range=(0, 1))
    scalerY=MinMaxScaler(feature_range=(-1,1))

    # Scale both the training X inputs and outputs
    scaled_training_X = scalerX.fit_transform(train_X)
    scaled_testing_X = scalerX.transform(test_X)

    train_Y=pd.DataFrame(train_Y,columns=[dependant])
    test_Y=pd.DataFrame(test_Y,columns=[dependant])
    scaled_training_Y=scalerY.fit_transform(train_Y)
    scaled_testing_Y=scalerY.transform(test_Y)

    # Create new pandas DataFrame objects from the scaled data
    scaled_training_X = pd.DataFrame(scaled_training_X, columns=train_X.columns.values)
    scaled_testing_X = pd.DataFrame(scaled_testing_X, columns=test_X.columns.values)

    scaled_training_Y = pd.DataFrame(scaled_training_Y, columns=train_Y.columns.values)
    scaled_testing_Y = pd.DataFrame(scaled_testing_Y, columns=test_Y.columns.values)

    input_dim=len(scaled_training_X.columns)

    return scaled_training_X, scaled_testing_X, scaled_training_Y, scaled_testing_Y, input_dim,scalerY,dependant

def feature_selection(raw_data,historical,dependant):
    data=raw_data
    dependant=dependant
    historical=historical
    #print(historical.columns)
    scalerX = MinMaxScaler(feature_range=(-1000000, 1000000))

    # Scale both the training X inputs and outputs

    data_transformed = scalerX.fit_transform(data)


    data_transformed = pd.DataFrame(data_transformed, columns=data.columns.values)
    data_transformed=data_transformed.astype(int)
    Feature_selection=pd.DataFrame()

    for x in data.columns:
        #print(x)

        X=data_transformed[[x]].values.reshape(-1,1)
        historical=historical[[dependant]]


        scalerY = MinMaxScaler(feature_range=(-1000000, 1000000))
        historical_transformed=scalerY.fit_transform(historical)
        historical_transformed = pd.DataFrame(historical_transformed, columns=historical.columns.values)
        y=historical_transformed.astype(int).values.ravel()

        #chi2_output=chi2(X,y)
        #chi2_output=float(chi2_output[0])


        mutual_info=mutual_info_classif(X,y)
        mutual_info=float(mutual_info)

        #data_transformed2=data_transformed.tail(30)

        #X=data_transformed2[[x]].values.reshape(-1,1)
        #y=data_transformed2[dependant]

        #explained_variance=explained_variance_score(X,y)

        prelim_result=pd.DataFrame({'mutual_info':[mutual_info],'column_name':[x]})

        Feature_selection=Feature_selection.append(prelim_result)
    Feature_selection=Feature_selection.sort_values(by=['mutual_info'],ascending=False)
    Feature_selection=Feature_selection[Feature_selection.column_name != dependant]
    Feature_selected=Feature_selection['column_name'].head(120)



    return Feature_selected

def build_train_model(density, input_dim, activation, output_activation, optimizer):

    model=Sequential()

    model.add(Dense(int(density*0.5), input_dim=input_dim, activation=activation))

    model.add(Dense(int(density*0.75), activation=activation))

    model.add(Dropout(0.3))

    model.add(Dense(int(density*1.2), activation=activation))

    model.add(Dense(int(density*2.6), activation=activation))

    model.add(Dense(int(density*4.2), activation=activation))

    model.add(Dense(int(density*3.6), activation=activation))

    model.add(Dense(int(density*2.6), activation=activation))

    model.add(Dense(int(density*4.2), activation=activation))

    model.add(Dense(int(density*2.2), activation=activation))

    model.add(Dense(int(density*1.6), activation=activation))

    model.add(Dropout(0.3))

    model.add(Dense(int(density*0.5), activation=activation))

    model.add(Dense(1, activation=output_activation))

    model.compile(loss="logcosh", optimizer=optimizer)

    return model

def model_backtest(model,scaled_testing_Y,scaled_testing_X,scalerY,dependant):

    Y_test = scaled_testing_Y
    X_test = scaled_testing_X

    predictions=model.predict(X_test)
    predictions=scalerY.inverse_transform(predictions)
    predictions=pd.DataFrame(predictions,columns=['{} predictions'.format(dependant)])

    Y_test=scalerY.inverse_transform(Y_test)
    error=np.absolute((predictions)-Y_test)

    mean_absolute_error=np.mean(error)
    mean_absolute_values=np.mean(np.abs(Y_test))

    print(mean_absolute_error)

    print(mean_absolute_values)

    average_absolute_percentage_error=(mean_absolute_error/mean_absolute_values)*100
    print(average_absolute_percentage_error)


def train_model():
    client_s3 = boto3.resource("s3")

    client_s3.Bucket('plt-ukgas-dev-use1-hubbalance-s3').download_file('Interconnector_Model/Training_Data/HyperParameter_Setting.json','HyperParameter_Setting.json')
    config=pd.read_json('HyperParameter_Setting.json')

    Bucket=config['Bucket'].iloc[0]
    dependant=str(config['FlowItem'].iloc[0])
    HistoricalFile=config['HistoricalFile'].iloc[0]
    HistoricalFileName=config['HistoricFileName'].iloc[0]
    ForecastFile=config['ForecastFile'].iloc[0]
    ForecastFileName=config['ForecastFileName'].iloc[0]
    OutputFilePath=config['OutputPath'].iloc[0]

    #feature_selection_number=config['FeatureSelection'].iloc[0]
    Model_Location=Bucket+'/'+OutputFilePath




    client_s3.Bucket(Bucket).download_file(ForecastFile,ForecastFileName)
    forecast=pd.read_csv(ForecastFileName)

    del forecast['gas_day']; del forecast['weather_applicable_for_start'];


    client_s3.Bucket(Bucket).download_file(HistoricalFile,HistoricalFileName)
    historical=pd.read_csv(HistoricalFileName)

    historical.columns
    del historical['gas_day']; del historical['weather_applicable_for_start']
    forecast=forecast.dropna()
    historical=historical.fillna(0.1)

    #Create forecast point list
    historical_allocations_data_columns=pd.DataFrame(historical.columns,columns=['Columns'])
    forecast_data_columns=pd.DataFrame(forecast.columns,columns=['Columns'])

    mergedStuff = historical_allocations_data_columns.merge(forecast_data_columns, on=['Columns'], how='left',indicator=True)

    train_columns=mergedStuff['_merge']=='both'

    key=OutputFilePath+ 'train_columns.csv'

    try:
        client_s3.Bucket(Bucket).download_file(key,'train_columns.csv')

    except botocore.exceptions.ClientError as e:
        print('error')
        train_columns=mergedStuff['_merge']=='both'
        train_columns=mergedStuff[train_columns]['Columns']
        train_X=historical[forecast.columns.intersection(train_columns)]
        train_columns=train_columns.append(pd.Series(dependant),ignore_index=True)
        train_columns=pd.DataFrame(train_columns,columns=['test'])
        train_columns.to_csv('train_columns.csv')
        boto3.Session().resource('s3').Bucket(Bucket).Object(OutputFilePath+ 'train_columns.csv').upload_file('train_columns.csv')

    else:
        print('done')
        train_columns=pd.read_csv('train_columns.csv')
        train_columns=pd.Series(train_columns['test'])
        train_X=historical[forecast.columns.intersection(train_columns)]
        train_columns=train_columns.append(pd.Series(dependant),ignore_index=True)
        train_columns=pd.DataFrame(train_columns,columns={'test'})
        train_columns.to_csv('train_columns.csv')
        boto3.Session().resource('s3').Bucket(Bucket).Object(OutputFilePath+ 'train_columns.csv').upload_file('train_columns.csv')

    #train_columns=mergedStuff[train_columns]['Columns']
    #train_X=historical[forecast.columns.intersection(train_columns)]
    #train_columns=pd.Series(a)
    #train_columns_dependant=mergedStuff[train_columns]['Columns']

    #print(train_columns)

    #train_X=historical[forecast.columns.intersection(train_columns_dependant)]
    #testlist=testlist.iloc[1:]

    features_selected=feature_selection(train_X,historical,dependant)

    train_X=train_X[features_selected]
    train_Y=historical[dependant]

    scaled_training_X, scaled_testing_X, scaled_training_Y, scaled_testing_Y, input_dim,scalerY,dependant=data_process(
        train_X,dependant,train_Y)
    input_dim=input_dim

    density=1500
    activation='relu'
    output_activation='linear'
    optimizer='SGD'

    model= build_train_model(density,input_dim,activation,output_activation,optimizer)

    optimized_model=model.fit(scaled_training_X,
             scaled_training_Y,
             epochs=1,
             shuffle=True,
             verbose=1)

    features_selected=pd.DataFrame(features_selected,columns=['column_name'])
    optimized_model.model.save('{}.tar.gz'.format(dependant))
    features_selected.to_csv('{} selected features.csv'.format(dependant))

    boto3.Session().resource('s3').Bucket(Bucket).Object(OutputFilePath+'{}.tar.gz'.format(dependant)).upload_file('{}.tar.gz'.format(dependant))
    boto3.Session().resource('s3').Bucket(Bucket).Object(OutputFilePath+'{} selected features.csv'.format(dependant)).upload_file('{} selected features.csv'.format(dependant))

    return Model_Location

if __name__=='__main__':

    try:
        Model_Location=train_model()
        print('done')
    except Exception as e:
        print('error')
